[MUSIC PLAYING]

PROFESSOR: Well, Hal just told us how you build robust systems. The key idea was-- I'm surethat many of you don't really assimilate that yet-- but the key idea is that in order to make asystem that's robust, it has to be insensitive to small changes, that is, a small change in theproblem should lead to only a small change in the solution. There ought to be a continuity. Thespace of solutions ought to be continuous in this space of problems.

The way he was explaining how to do that was instead of solving a particular problem at everylevel of decomposition of the problem at the subproblems, where you solve the class ofproblems, which are a neighborhood of the particular problem that you're trying to solve. Theway you do that is by producing a language at that level of detail in which the solutions to thatclass of problems is representable in that language. Therefore when you makes more changesto the problem you're trying to solve, you generally have to make only small local changes to thesolution you've constructed, because at the level of detail you're working, there's a languagewhere you can express the various solutions to alternate problems of the same type.

Well that's the beginning of a very important idea, the most important perhaps idea that makescomputer science more powerful than most of the other kinds of engineering disciplines we knowabout. What we've seen so far is sort of how to use embedding of languages. And, of course, thepower of embedding languages partly comes from procedures like this one that I showed youyesterday.

What you see here is the derivative program that we described yesterday. It's a procedure thattakes a procedure as an argument and returns a procedure as a value. And using such things isvery nice. You can make things like push combinators and all that sort of wonderful thing thatyou saw last time.

However, now I'm going to really muddy the waters. See this confuses the issue of what's theprocedure and what is data, but not very badly. What we really want to do is confuse it verybadly. And the best way to do that is to get involved with the manipulation of the algebraic expressions that the procedures themselves are expressed in.

So at this point, I want to talk about instead of things like on this slide, the derivative procedurebeing a thing that manipulates a procedure-- this is a numerical method you see here. And whatyou're seeing is a representation of the numerical approximation to the derivative. That's what'shere. In fact what I'd like to talk about is instead things that look like this. And what we have hereare rules from a calculus book.

These are rules for finding the derivatives of the expressions that one might write in somealgebraic language. It says things like a derivative of a constant is 0. The derivative of thevaluable with respect to which you are taking the derivative is 1. The derivative of a constant times the function is the constant times the derivative of the function, and things like that. Theseare exact expressions. These are not numerical approximations. Can we make programs? And,in fact, it's very easy to make programs that manipulate these expressions.

Well let's see. Let's look at these rules in some detail. You all have seen these rules in yourelementary calculus class at one time or another. And you know from calculus that it's easy toproduce derivatives of arbitrary expressions. You also know from your elementary calculus that it's hard to produce integrals. Yet integrals and derivatives are opposites of each other. They'reinverse operations. And they have the same rules. What is special about these rules that makesit possible for one to produce derivatives easily and integrals why it's so hard? Let's think aboutthat very simply.

Look at these rules. Every one of these rules, when used in the direction for taking derivatives,which is in the direction of this arrow, the left side is matched against your expression, and theright side is the thing which is the derivative of that expression. The arrow is going that way. Ineach of these rules, the expressions on the right-hand side of the rule that are contained withinderivatives are subexpressions, are proper subexpressions, of the expression on the left-handside.

So here we see the derivative of the sum, with is the expression on the left-hand side is the sumof the derivatives of the pieces. So the rule of moving to the right are reduction rules. Theproblem becomes easier. I turn a big complicated problem it's lots of smaller problems and thencombine the results, a perfect place for recursion to work.

If I'm going in the other direction like this, if I'm trying to produce integrals, well there are severalproblems you see here. First of all, if I try to integrate an expression like a sum, more than onerule matches. Here's one that matches. Here's one that matches. I don't know which one to take.And they may be different. I may get to explore different things. Also, the expressions becomelarger in that direction. And when the expressions become larger, then there's no guarantee thatany particular path I choose will terminate, because we will only terminate by accidentalcancellation. So that's why integrals are complicated searches and hard to do.

Right now I don't want to do anything as hard as that. Let's work on derivatives for a while. Well,these roles are ones you know for the most part hopefully. So let's see if we can write a programwhich is these rules. And that should be very easy. Just write the program. See, because while Ishowed you is that it's a reduction rule, it's something appropriate for a recursion. And, ofcourse, what we have for each of these rules is we have a case in some case analysis.

So I'm just going to write this program down. Now, of course, I'm going to be saying something you have to believe. Right? What you have to believe is I can represent these algebraic expressions, that I can grab their parts, that I can put them together. We've invented liststructures so that you can do that. But you don't want to worry about that now. Right now I'm going to write the program that encapsulates these rules independent of the representation of the algebraic expressions.

---

07:16

You have a derivative of an expression with respect to a variable. This is a different thing than the derivative of the function. That's what we saw last time, that numerical approximation. It's something you can't open up a function. It's just the answers. The derivative of an expression is the way it's written. And therefore it's a syntactic phenomenon. And so a lot of what we're goingto be doing today is worrying about syntax, syntax of expressions and things like that.

Well, there's a case analysis. Anytime we do anything complicated thereby a recursion, we presumably need a case analysis. It's the essential way to begin. And that's usually a conditional of some large kind. Well, what are their possibilities? the first rule that you saw is this something a constant? And what I'm asking is, is the expression a constant with respect to the variablegiven? If so, the result is 0, because the derivative represents the rate of change of something.

If, however, the expression that I'm taking the derivative of is the variable I'm varying, then this is the same variable, the expression var, then the rate of change of the expression with respect to the variable is 1. It's the same 1.

Well now there are a couple of other possibilities. It could, for example, be a sum. Well, I don'tknow how I'm going to express sums yet. Actually I do. But I haven't told you yet. But is it a sum?I'm imagining that there's some way of telling. I'm doing a dispatch on the type of the expressionhere, absolutely essential in building languages. Languages are made out of differentexpressions. And soon we're going to see that in our more powerful methods of buildinglanguages on languages.

Is an expression a sum? If it's a sum, well, we know the rule for derivative of the sum is the sumof the derivatives of the parts. One of them is called the addend and the other is the augend. ButI don't have enough space on the blackboard to such long names. So I'll call them A1 and A2.

I want to make a sum. Do you remember which is the sum for end or the menu end? Or was itthe dividend and the divisor or something like that? Make sum of the derivative of the A1, I'll callit. It's the addend of the expression with respect to the variable, and the derivative of the A2 ofthe expression, because the two arguments, the addition with respect to the variable.

And another rule that we know is product rule, which is, if the expression is a product. By theway, it's a good idea when you're defining things, when you're defining predicates, to give them a name that ends in a question mark. This question mark doesn't mean anything. It's for us as an agreement. It's a conventional interface between humans so you can read my programs more easily. So I want you to, when you write programs, if you define a predicate procedure, that'ssomething that rings true of false, it should have a name which ends in question mark. The listdoesn't care. I care.

I want to make a sum. Because the derivative of a product is the sum of the first times the derivative of the second plus the second times the derivative of the first. Make a sum of twothings, a product of, well, I'm going to say the M1 of the expression, and the derivative of the M2 of the expression with respect to the variable, and the product of the derivative of M1, the multiplier of the expression, with respect to the variable. It's the product of that and themultiplicand, M2, of the expression. Make that product. Make the sum. Close that case.

---

12:30

And, of course, I could add as many cases as I like here for a complete set of rules you might find in a calculus book. So this is what it takes to encapsulate those rules. And you see, youhave to realize there's a lot of wishful thinking here. I haven't told you anything about how I'm going to make these representations. Now, once I've decided that this is my set of rules, I thinkit's time to play with the representation. Let's attack that/

Well, first of all, I'm going to play a pun. It's an important pun. It's a key to a sort of powerful idea.If I want to represent sums, and products, and differences, and quotients, and things like that,why not use the same language as I'm writing my program in? I write my program in algebraic expressions that look like the sum of the product on a and the product of x and x, and things likethat. And the product of b and x and c, whatever, make that a sum of the product. Right now Idon't want to have procedures with unknown numbers of arguments, a product of b and x and c.

This is list structure. And the reason why this is nice, is because any one of these objects has a property. I know where the car is. The car is the operator. And the operands are the successive cdrs the successive cars of the cdrs of the list that this is. It makes it very convenient. I have toparse it. It's been done for me. I'm using the embedding and Lisp to advantage.

So, for example, let's start using list structure to write down the representation that I'm implicitlyassuming here. Well I have to define various things that are implied in this representation. Like Ihave to find out how to do a constant, how you do same variable. Let's do those first. That'spretty easy enough. Now I'm going to be introducing lots of primitives here, because these are the primitives that come with list structure.

OK, you define a constant. And what I mean by a constant, an expression that's constant with respect to a veritable, is that the expression is something simple. I can't take it into pieces, andyet it isn't that variable. I can't break it up, and yet it isn't that variable. That does not mean that there may be other expressions that are more complicated that are constants. It's just that I'm going to look at the primitive constants in this way.

So what this is, is it says that's it's the and. I can combine predicate expressions which returntrue or false with and. Something atomic, The expression is atomic, meaning it cannot be brokeninto parts. It doesn't have a car and a cdr. It's not a list. It adds a special test built into thesystem. And it's not identically equal to that variable. I'm representing my variable by things thatare symbols which cannot be broken into pieces, things like x, and y, things like this. Whereas, ofcourse, something like this can be broken up into pieces.

And the same variable of an expression with respect to a variable is, in fact, an atomicexpression. I want to have an atomic expression, which is identical. I don't want to look insidethis stuff anymore. These are primitive maybe. But it doesn't matter. I'm using things that aregiven to me with a language. I'm not terribly interest in them

Now how do we deal with sums? Ah, something very interesting will happen. A sum is somethingwhich is not atomic and begins with the plus symbol. That's what it means. So here, I will define.An question is a sum if and it's not atomic and it's head, it's beginning, its car of the expression isthe symbol plus.

Now you're about to see something you haven't seen before, this quotation. Why do I have that quotation there? Say your name,

AUDIENCE: Susanna.

PROFESSOR: Louder.

AUDIENCE: Susanna

PROFESSOR: Say your name.

AUDIENCE: Your name.

PROFESSOR: Louder.

AUDIENCE: Your name.

PROFESSOR: OK. What I'm showing you here is that the words of English are ambiguous. I was saying, say your name. I was also possibly saying say, your name. But that cannot bedistinguished in speech. However, we do have a notation in writing, which is quotation for distinguishing these two possible meanings.

In particular, over here, in Lisp we have a notation for distinguishing these meetings. If I were to just write a plus here, a plus symbol, I would be asking, is the first element of the expression, is the operator position of the expression, the addition operator? I don't know. I would have to have written the addition operator there, which I can't write. However, this way I'm asking, is this the symbolic object plus, which normally stands for the addition operator? That's what I want. That's the question I want to ask. Now before I go any further, I want to point out the quotation is a very complex concept, and adding it to a language causes a great deal of troubles.

Consider the next slide. Here's a deduction which we should all agree with. We have, Alyssa issmart and Alyssa is George's mother. This is an equality, is. From those two, we can deduce thatGeorge's mother is smart. Because we can always substitute equals for equals in expressions.Or can we?

Here's a case where we have "Chicago" has seven letters. The quotation means that I'm discussing the word Chicago, not what the word represents. Here I have that Chicago is thebiggest city in Illinois. As a consequence of this, I would like to deduce that the biggest city inIllinois has seven letters. But that's manifestly false. Wow, it works.

OK, so once we have things like that, our language gets much more complicated. Because it's no longer true that things we tend to like to do with languages, like substituting equals for equalsand getting right answers, are going to work without being very careful. We can't substitute into what's called referentially opaque contexts, of which a quotation is the prototypical type of referentially opaque context. If you know what that means, you can consult a philosopher.Presumably there is one in the room.

---

21:33

In any case, let's continue now, now that we at least have an operational understanding of a2000-year-old issue that has to do with name, and mention, and all sorts of things like that. Ihave to define what I mean, how to make a sum of two things, an a1 and a2. And I'm going to do this very simply. It's a list of the symbol plus, and a1, and a2. And I can determine the firstelement. Define a1 to be cadr. I've just introduced another primitive. This is the car of the cdr ofsomething.

You might want to know why car and cdr are names of these primitives, and why they'vesurvived, even though they're much better ideas like left and right. We could have called themthings like that. Well, first of all, the names come from the fact that in the great past, when Lispwas invented, I suppose in '58 or something, it was on a 704 or something like that, which had amachine. It was a machine that had an address register and a decrement register. And thesewere the contents of the address register and the decrement register. So it's an historicalaccident.

Now why have these names survived? It's because Lisp programmers like to talk to each otherover the phone. And if you want to have a long sequence of cars and cdrs you might say,cdaddedr, which can be understood. But left of right or right of left is not so clear if you get goodat it. So that's why we have these words. All of them up to four deep are defined typically in a Lisp system. A2 to be-- and, of course, you can see that if I looked at one of these expressionslike the sum of 3 and 5, what that is is a list containing the symbol plus, and a number 3, and anumber 5. Then the car is the symbol plus. The car of the cdr. Well I take the cdr and then I takethe car. And that's how I get to the 3. That's the first argument. And the car of the cdr of the cdrgets me to this one, the 5.

And similarly, of course, I can define what's going on with products. Let's do that very quickly. Isthe expression a product? Yes if and if it's true, that's it's not atomic and it's EQ quote, theasterisk symbol, which is the operator for multiplication.

Make product of an M1 and an M2 to be list, quote, the asterisk operation and M1 and M2. and Idefine M1 to be cadr and M2 to be caddr. You get to be a good Lisp programmer because youstart talking that way. I cdr down lists and console them up and so on.

Now, now that we have essentially a complete program for finding derivatives, you can add morerules if you like. What kind of behavior do we get out of it? I'll have to clear that x. Well,supposing I define foo here to be the sum of the product of ax square and bx plus c. That's thesame thing we see here as the algebraic expression written in the more conventional notationover there.

Well, the derivative of foo with respect to x, which we can see over here, is this horrible,horrendous mess. I would like it to be 2ax plus b. But it's not. It's equivalent to it. What is it? Ihave here, what do I have? I have the derivative of the product of x and x. Over here is, ofcourse, the sum of x times 1 and 1 times x.

Now, well, it's the first times the derivative of the second plus the second times the derivative of the first. It's right. That's 2x of course. a times 2x is 2ax plus 0X square doesn't count plus B overhere plus a bunch of 0's. Well the answer is right. But I give people take off points on an examfor that, sadly enough. Let's worry about that in the next segment. Are there any questions?Yes?

AUDIENCE: If you had left the quote when you put the plus, then would that be referring to theprocedure plus and could you do a comparison between that procedure and some otherprocedure if you wanted to?

PROFESSOR: Yes. Good question. If I had left this quotation off at this point, if I had left thatquotation off at that point, then I would be referring here to the procedure which is the thing thatplus is defined to be.

And indeed, I could compare some procedures with each other for identity. Now what that meansis not clear right now. I don't like to think about it. Because I don't know exactly what it wouldneed to compare procedures. There are reasons why that may make no sense at all. However,the symbols, we understand. And so that's why I put that quote in. I want to talk about thesymbol that's apparent on the page. Any other questions? OK. Thank you. Let's take a break.

### Part 2

[MUSIC PLAYING]

PROFESSOR: Well, let's see. We've just developed a fairly plausible program for computing thederivatives of algebraic expressions. It's an incomplete program, if you would like to add morerules. And perhaps you might extend it to deal with uses of addition with any number ofarguments and multiplication with any of the number of arguments. And that's all rather easy.

However, there was a little fly in that ointment. We go back to this slide. We see that the expressions that we get are rather bad. This is a rather bad expression. How do we get such anexpression? Why do we have that expression? Let's look at this expression in some detail. Let'sfind out where all the pieces come from. As we see here, we have a sum-- just what I showedyou at the end of the last time-- of X times 1 plus 1 time X. That is a derivative of this product.The produce of a times that, where a does not depend upon x, and therefore is constant withrespect to x, is this sum, which goes from here all the way through here and through here.Because it is the first thing times the derivative of the second plus the derivative of the first timesthe second as the program we wrote on the blackboard indicated we should do.

And, of course, the product of bx over here manifests itself as B times 1 plus 0 times X becausewe see that B does not depend upon X. And so the derivative of B is this 0, and the derivative of X with respect itself is the 1. And, of course, the derivative of the sums over here turn into thesetwo sums of the derivatives of the parts.

So what we're seeing here is exactly the thing I was trying to tell you about with Fibonacci numbers a while ago, that the form of the process is expanded from the local rules that you see in the procedure, that the procedure represents a set of local rules for the expansion of this process. And here, the process left behind some stuff, which is the answer. And it was constructed by the walk it takes of the tree structure, which is the expression. So every part inthe answer we see here derives from some part of the problem.

Now, we can look at, for example, the derivative of foo, which is ax square plus bx plus c, withrespect to other things, like here, for example, we can see that the derivative of foo with respectto a. And it's very similar. It's, in fact, the identical algebraic expression, except for the fact thattheses 0's and 1's are in different places. Because the only degree of freedom we have in thistree walk is what's constant with respect to the variable we're taking the derivative with respectto and was the same variable.

In other words, if we go back to this blackboard and we look, we have no choice what to dowhen we take the derivative of the sum or a product. The only interesting place here is, is theexpression the variable, or is the expression a constant with respect to that variable for very,very small expressions? In which case we get various 1's and 0's, which if we go back to thisslide, we can see that the 0's that appear here, for example, this 1 over here in derivative of foowith respect to A, which gets us an X square, because that 1 gets the multiply of X and X into theanswer, that 1 is 0. Over here, we're not taking the derivative of foo with respect to c. But theshapes of these expressions are the same. See all those shapes. They're the same.

Well is there anything wrong with our rules? No. They're the right rules. We've been through thisone before. One of the things you're going to begin to discover is that there aren't too many goodideas. When we were looking at rational numbers yesterday, the problem was that we got 6/8 rather then 3/4. The answer was unsimplified. The problem, of course, is very similar. There arethings I'd like to be identical by simplification that don't become identical. And yet the rules for doing addition a multiplication of rational numbers were correct.

So the way we might solve this problem is do the thing we did last time, which always works. If something worked last time it ought to work again. It's changed representation. Perhaps in therepresentation we could put in a simplification step that produces a simplified representation.This may not always work, of course. I'm not trying to say that it always works. But it's one of the pieces of artillery we have in our war against complexity.

You see, because we solved our problem very carefully. What we've done, is we've divided theworld in several parts. There are derivatives rules and general rules for algebra of some sort atthis level of detail. and i have an abstraction barrier. And i have the representation of the algebraic expressions, list structure.

And in this barrier, I have the interface procedures. I have constant, and things like same-var. Ihave things like sum, make-sum. I have A1, A2. I have products and things like that, all the otherthings I might need for various kinds of algebraic expressions.

Making this barrier allows me to arbitrarily change the representation without changing the rulesthat are written in terms of that representation. So if I can make the problem go away bychanging representation, the composition of the problem into these two parts has helped me agreat deal.

So let's take a very simple case of this. What was one of the problems? Let's go back to thistransparency again. And we see here, oh yes, there's horrible things like here is the sum of anexpression and 0. Well that's no reason to think of it as anything other than the expression itself.Why should the summation operation have made up this edition? It can be smarter than that. Orhere, for example, is a multiplication of something by 1. It's another thing like that. Or here is aproduct of something with 0, which is certainly 0. So we won't have to make this construction.

So why don't we just do that? We need to change the way the representation works, almosthere. Make-sum to be. Well, now it's not something so simple. I'm not going to make a listcontaining the symbol plus and things unless I need to.

Well, what are the possibilities? I have some sort of cases here. If I have numbers, if anyone is anumber-- and here's another primitive I've just introduced, it's possible to tell whethersomething's number-- and if number A2, meaning they're not symbolic expressions, then whynot do the addition now? The result is just a plus of A1 and A2.

I'm not asking if these represent numbers. Of course all of these symbols represent numbers. I'mtalking about whether the one I've got is the number 3 right now. And, for example, supposing A1is a number, and it's equal to 0, well then the answer is just A2. There is no reason to makeanything up. And if A2 is a number, and equal A20, then the result is A1.

And only if I can't figure out something better to do with this situation, well, I can start a list.Otherwise I want the representation to be the list containing the quoted symbol plus, and A1, andA2.

And, of course, a very similar thing can be done for products. And I think I'll avoid boring youwith them. I was going to write it on the blackboard. I don't think it's necessary. You know what todo. It's very simple.

But now, let's just see the kind of results we get out of changing our program in this way. Well,here's the derivatives after having just changed the constructors for expressions. The same foo,aX square plus bX plus c, and what I get is nothing more than the derivative of that is 2aX plusB.

Well, it's not completely simplified. I would like to collect common terms and sums. Well, that'smore work. And, of course, programs to do this sort of thing are huge and complicated. Algebraic simplification, it's a very complicated mess. There's a very famous program you may have heard of called Maxima developed at MIT in the past, which is 5,000 pages of Lisp code, mostly the algebraic simplification operations.

---

41:03

There we see the derivative of foo. In fact, X is at something I wouldn't take off more than 1 pointfor on an elementary calculus class. And the derivative of foo with respect to a, well it's gonedown to X times X, which isn't so bad. And the derivative of foo with respect to b is just X itself.And the derivative of foo with respect to c comes out 1. So I'm pretty pleased with this.

What you've seen is, of course, a little bit contrived, carefully organized example to show you how we can manipulate algebraic expressions, how we do that abstractly in terms of abstract syntax rather than concrete syntax and how we can use the abstraction to control what goes on in building these expressions.

But the real story isn't just such a simple thing as that. The real story is, in fact, that I'm manipulating these expressions. And the expressions are the same expressions-- going back to the slide-- as the ones that are Lisp expressions. There's a pun here. I've chosen my representation to be the same as the representation in my language of similar things. By doing so, I've invoked a necessity. I created the necessity to have things like quotation because of the fact that my language is capable of writing expressions that talk about expressions of the language. I need to have something that says, this is an expression I'm talking about rather than this expression is talking about something, and I want to talk about that.

So quotation stops and says, I'm talking about this expression itself. Now, given that power, if I can manipulate expressions of the language, I can begin to build even much more powerful layers upon layers of languages. Because I can write languages that not only are embedded in Lisp or whatever language you start with, but languages that are completely different, that are just, if we say, interpreted in Lisp or something like that.

We'll get to understand those words more in the future. But right now I just want to leave you with the fact that we've hit a line which gives us tremendous power. And this point we've bought a sledgehammer. We have to be careful to what flies when we apply it. Thank you.

[MUSIC PLAYING]